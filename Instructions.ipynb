{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-53533843055c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-53533843055c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    This jupyter notebook includes instructions about how to use `situational awareness` tool analyzing time series data.\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "This jupyter notebook includes instructions about how to use `situational awareness` tool analyzing time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from isoweek import Week\n",
    "from optimization import situational_awareness as sa\n",
    "#reload(sa)\n",
    "from optimization import problem\n",
    "from optimization import filter_selection as fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each data source is converted to the same format, and sotred in a list.\n",
    "**NOTE**: change "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A module to combine Data format, Preprocess data, and Optimization for situational awareness\n",
    "\n",
    "- This module will use each csv file in `GoldStandard` as gold standard, and all time series in each csv file in `SourceToOptimize` as candidate data sources.\n",
    "\n",
    "- It will write optimization results into new csv files and save in provided folder.\n",
    "\n",
    "- Use this module for automated optimization process.\n",
    "\n",
    "**NOTE**: All input csv files should have the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.2'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Remy\\Desktop\\UT Austin\\Meyers Lab\\Venezuela Situational Awareness\\RemySituAwareness\\optimization\n",
      "C:\\Users\\Remy\\Desktop\\UT Austin\\Meyers Lab\\Venezuela Situational Awareness\\RemySituAwareness\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from isoweek import Week\n",
    "import pandas as pd\n",
    "\n",
    "if os.getcwd()[-12:] != 'optimization':\n",
    "    os.chdir(os.getcwd() + '\\\\optimization\\\\')\n",
    "import problem\n",
    "import filter_selection as fs\n",
    "import situational_awareness as sa\n",
    "os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "def _to_json(_id, _times, _values):\n",
    "    \"\"\"\n",
    "    Generate a dict\n",
    "    \"\"\"\n",
    "    data_source = {'id': _id,\n",
    "                   'data':\n",
    "                   {'times': _times,\n",
    "                    'values': _values}\n",
    "                  }\n",
    "    return data_source\n",
    "\n",
    "def csv_to_json(file_path):\n",
    "    \"\"\"\n",
    "    Convert data from csv format to Json format\n",
    "    \"\"\"\n",
    "    # read csv\n",
    "    csv_data = pd.read_csv(file_path)\n",
    "    \n",
    "    # check value in column 'year/week' is date or year/week\n",
    "    if len(str(csv_data.loc[0, 'year/week']).split('/')) > 1:\n",
    "        csv_data.loc[:, 'date'] = csv_data.loc[:, 'year/week'].apply(pd.to_datetime)\n",
    "    else:\n",
    "        # convert year /week to date\n",
    "        csv_data.loc[:, 'year'] = [i[:4] for i in csv_data.loc[:, 'year/week'].apply(str)]\n",
    "        csv_data.loc[:, 'week'] = [i[-2:] for i in csv_data.loc[:, 'year/week'].apply(str)]\n",
    "        csv_data.loc[:, 'date'] = [pd.to_datetime(Week(int(csv_data.loc[i, 'year']),\n",
    "                                        int(csv_data.loc[i, 'week'])).sunday()) for i in csv_data.index.values]\n",
    "    \n",
    "    # write into json\n",
    "    column_names = csv_data.columns\n",
    "    all_data = []\n",
    "    for i in range(csv_data.shape[1]):\n",
    "        if column_names[i] in ['year/week', 'year', 'week', 'date']:\n",
    "            pass\n",
    "        else:\n",
    "            _id = column_names[i].replace('-', '_')\n",
    "            _times = csv_data.loc[:, 'date'].tolist()\n",
    "            _values = csv_data.loc[:, column_names[i]].tolist()\n",
    "            data_source = {'id': _id,\n",
    "                           'data':\n",
    "                           {'times': _times,\n",
    "                            'values': _values}\n",
    "                          }\n",
    "            all_data.append(data_source)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def filter_data(data,\n",
    "                date_start='2010-11-22',\n",
    "                date_end='2016-09-19'):\n",
    "    \"\"\"\n",
    "    Filter data to make sure all data are within the same date range\n",
    "    \"\"\"\n",
    "    for i in range(len(data)):\n",
    "        bool_mask = fs.filter_on_times(data[i], after=date_start, before=date_end)\n",
    "        d = fs.select(data[i], bool_mask)\n",
    "        data[i] = d\n",
    "    return data\n",
    "\n",
    "\n",
    "def optimization(gold_standard,\n",
    "                 candidate_data_sources,\n",
    "               objective='R_squared',\n",
    "               n_folds=15,\n",
    "               output_size=141):\n",
    "    \"\"\"\n",
    "    Run optimization for situational awareness.\n",
    "    \n",
    "    candidate_data_sources: list\n",
    "    gold_standard: dict\n",
    "    \"\"\"\n",
    "    # optimization\n",
    "    sa_optimization = problem.FS_problem(gold_standard,\n",
    "                                         candidate_data_sources,\n",
    "                                         req_data=[],\n",
    "                                         objective=objective,\n",
    "                                         n_folds=n_folds)\n",
    "    optimum, objective_value, objective_single = sa_optimization.optimize('forward_selection', choose=output_size)\n",
    "    # predictions based on optimum\n",
    "    prediction = sa.pred_CV(gold_standard, optimum)['data']\n",
    "    return optimum, objective_value, objective_single, prediction\n",
    "\n",
    "\n",
    "def main(gold_standard_folder,\n",
    "         candidate_folder,\n",
    "         date_start='2010-11-22',\n",
    "         date_end='2016-09-19',\n",
    "         objective='R_squared',\n",
    "         n_folds=15,\n",
    "         output_size=141,\n",
    "         save_folder='OptimizationResults'):\n",
    "    # retrieve all csv files in each folder\n",
    "    gold_standard_paths = glob.glob(os.path.join(gold_standard_folder, '*'))\n",
    "    candidate_paths = glob.glob(os.path.join(candidate_folder, '*'))\n",
    "    \n",
    "    for g_path in gold_standard_paths:\n",
    "        # load gold standard\n",
    "        print('Loading gold standard from {}'.format(g_path.split('/')[-1]))\n",
    "        gold_standard = csv_to_json(g_path)\n",
    "        print('Done loading gold standard.')\n",
    "        \n",
    "        for c_path in candidate_paths:\n",
    "            # load each csv file in SourceToOptimize\n",
    "            print('Loading candidate data sources from {}'.format(c_path.split('/')[-1]))\n",
    "            candidate_data_sources = csv_to_json(c_path)\n",
    "            print('Done loading candidate data sources.')\n",
    "            \n",
    "            # filter gold standard and candidate data sources\n",
    "            gold_standard = filter_data(gold_standard,\n",
    "                                        date_start=date_start,\n",
    "                                        date_end=date_end)\n",
    "            candidate_data_sources = filter_data(candidate_data_sources,\n",
    "                                                 date_start=date_start,\n",
    "                                                 date_end=date_end)\n",
    "            print('Done filtering data.')\n",
    "            \n",
    "            # optimization\n",
    "            optimum, objective_value, objective_single, prediction = optimization(gold_standard[0],\n",
    "                                                                                  candidate_data_sources,\n",
    "                                                                                  objective=objective,\n",
    "                                                                                  n_folds=n_folds,\n",
    "                                                                                  output_size=output_size)\n",
    "            # get name of gold standard csv and candidate data csv names.\n",
    "            g_name = g_path.split('/')[-1].split('.')[0]\n",
    "            c_name = c_path.split('/')[-1].split('.')[0]\n",
    "            \n",
    "            # write to csv\n",
    "            if not os.path.exists(save_folder):\n",
    "                os.makedirs(save_folder)\n",
    "                \n",
    "            # save performance of each single data source in candidate\n",
    "            ids = [d['id'] for d in candidate_data_sources]\n",
    "            df = pd.DataFrame(columns=['id', 'objective_values'])\n",
    "            df['id'] = ids\n",
    "            df['objective_values'] = objective_single\n",
    "            path = os.path.join(save_folder, '{}_{}_objective_single.csv'.format(g_name, c_name))\n",
    "            df.to_csv(path)\n",
    "\n",
    "            # save performance of optimal combination of data sources.\n",
    "            id_optimum = [i['id'] for i in optimum]\n",
    "            df = pd.DataFrame(columns=['id', 'objective_values'])\n",
    "            df['id'] = id_optimum\n",
    "            df['objective_values'] = objective_value[1:]\n",
    "            path = os.path.join(save_folder, '{}_{}_objective_optimum.csv'.format(g_name, c_name))\n",
    "            df.to_csv(path)\n",
    "\n",
    "            # save prediction results\n",
    "            df = pd.DataFrame(columns=['times', 'values'])\n",
    "            df['times'] = prediction['times']\n",
    "            df['values'] = prediction['values']\n",
    "            path = os.path.join(save_folder, '{}_{}_prediction.csv'.format(g_name, c_name))\n",
    "            df.to_csv(path)\n",
    "                                \n",
    "            print('Done optimization using gold standard from {} and candidate data from {}.\\n'.format(g_path.split('/')[-1], c_path.split('/')[-1]))\n",
    "    \n",
    "    print('Done all optimizations!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to do optimization.\n",
    "\n",
    "It will detect each csv files in the `gold_standard_folder` and `candidate_folder`.\n",
    "\n",
    "All optimization results (including optimized objective values, objective value for each single data source, out-of-sample prediction) will be saved to the `save_folder`.\n",
    "\n",
    "**NOTE**: to run these functions successfully, all csv files should have exactly the SAME format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gold standard from GoldStandard\\AMAZONAS-VE.csv\n",
      "Done loading gold standard.\n",
      "Loading candidate data sources from SourcesToOptimize\\ColombiaPlusGT.csv\n",
      "Done loading candidate data sources.\n",
      "Done filtering data.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=6 greater than the number of samples: n_samples=0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-9ecb6d375010>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m      \u001b[0mn_folds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m      \u001b[0moutput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m      save_folder='OptimizationResults')\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-af859f31655f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(gold_standard_folder, candidate_folder, date_start, date_end, objective, n_folds, output_size, save_folder)\u001b[0m\n\u001b[0;32m    131\u001b[0m                                                                                   \u001b[0mobjective\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                                                                                   \u001b[0mn_folds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m                                                                                   output_size=output_size)\n\u001b[0m\u001b[0;32m    134\u001b[0m             \u001b[1;31m# get name of gold standard csv and candidate data csv names.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mg_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-af859f31655f>\u001b[0m in \u001b[0;36moptimization\u001b[1;34m(gold_standard, candidate_data_sources, objective, n_folds, output_size)\u001b[0m\n\u001b[0;32m     87\u001b[0m                                          \u001b[0mobjective\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                                          n_folds=n_folds)\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0moptimum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjective_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjective_single\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msa_optimization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'forward_selection'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchoose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[1;31m# predictions based on optimum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred_CV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgold_standard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UT Austin\\Meyers Lab\\Venezuela Situational Awareness\\RemySituAwareness\\optimization\\problem.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, algorithm, **kwargs)\u001b[0m\n\u001b[0;32m    281\u001b[0m         \"\"\"\n\u001b[0;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimization_algs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m             \u001b[0msolution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"optimization_algs.\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolution\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimal_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolution\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UT Austin\\Meyers Lab\\Venezuela Situational Awareness\\RemySituAwareness\\optimization\\optimization_algs.py\u001b[0m in \u001b[0;36mforward_selection\u001b[1;34m(problem, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdatum\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mtemp_optimum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimum\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdatum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mobjective_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_optimum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mobjective_values_single_datum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjective_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UT Austin\\Meyers Lab\\Venezuela Situational Awareness\\RemySituAwareness\\optimization\\problem.py\u001b[0m in \u001b[0;36mobjective_function\u001b[1;34m(self, subset, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective_functions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             return eval(\"objective_functions.\" + self.\n\u001b[1;32m--> 138\u001b[1;33m                         objective)(self, subset, **kwargs)\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mearly_detection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             return eval(\"early_detection.\" + self.\n",
      "\u001b[1;32m~\\Desktop\\UT Austin\\Meyers Lab\\Venezuela Situational Awareness\\RemySituAwareness\\optimization\\objective_functions.py\u001b[0m in \u001b[0;36mR_squared\u001b[1;34m(problem, subset, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \"\"\"\n\u001b[0;32m     73\u001b[0m     pred_series_CV = sa.pred_CV(problem.goal, subset, problem.n_folds,\n\u001b[1;32m---> 74\u001b[1;33m                                 problem.bootstrap)['data']['values']\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[0mgoal_series\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproblem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgoal\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'values'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mnumerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtvar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgoal_series\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpred_series_CV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\UT Austin\\Meyers Lab\\Venezuela Situational Awareness\\RemySituAwareness\\optimization\\situational_awareness.py\u001b[0m in \u001b[0;36mpred_CV\u001b[1;34m(goal_datum, data_sources, n_folds, bootstrap)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;31m# kf = KFold(dm.length(goal_datum), n_folds)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mkf_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mkf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkf_p\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgoal_datum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgoal_datum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    331\u001b[0m                 (\"Cannot have number of splits n_splits={0} greater\"\n\u001b[0;32m    332\u001b[0m                  \" than the number of samples: n_samples={1}.\")\n\u001b[1;32m--> 333\u001b[1;33m                 .format(self.n_splits, n_samples))\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot have number of splits n_splits=6 greater than the number of samples: n_samples=0."
     ]
    }
   ],
   "source": [
    "main(gold_standard_folder='GoldStandard',\n",
    "     candidate_folder='SourcesToOptimize',\n",
    "      date_start='2005-01-02',\n",
    "     date_end='2009-12-27',\n",
    "#     date_start='2010-11-22',\n",
    "#     date_end='2013-09-19',\n",
    "     objective='R_squared',\n",
    "     n_folds=6,\n",
    "     output_size=10,\n",
    "     save_folder='OptimizationResults')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-sample evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same logic as optimization.\n",
    "\n",
    "- Need to put optimial combinations of predictor time series into one csv file, and put in a folder named 'SourcesForPrediction'\n",
    "\n",
    "- Specify the start and end dates of training period AND testing period in the function 'main_oos'\n",
    "\n",
    "- For each gold standard, the optimial combinations of predictor time series might be different. So each time running the experience, you should only put one gold standard in the 'GoldStandard' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from isoweek import Week\n",
    "import pandas as pd\n",
    "\n",
    "def oos_prediction(gold_standard,\n",
    "                   predictor_data_sources,\n",
    "                   train_period=['2004-01-01', '2006-01-01'],\n",
    "                   test_period=['2008-01-01', '2009-01-01']):\n",
    "    \"\"\"\n",
    "    Run out-of-sample evaluation for optimial models\n",
    "    \n",
    "    predictor_data_sources: list\n",
    "    gold_standard: dict\n",
    "    \"\"\"\n",
    "    # predictions based on optimum\n",
    "    prediction = sa.pred_OOS(gold_standard,\n",
    "                             predictor_data_sources,\n",
    "                             train_period,\n",
    "                             test_period)['data']\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def oos_main(gold_standard_folder,\n",
    "         predictor_folder,\n",
    "         date_start='2010-11-22',\n",
    "         date_end='2016-09-19',\n",
    "         train_period=['2004-01-01', '2006-01-01'],\n",
    "         test_period=['2008-01-01', '2009-01-01'],\n",
    "         save_folder='OOSPredictionResults'):\n",
    "    # retrieve all csv files in each folder\n",
    "    gold_standard_paths = glob.glob(os.path.join(gold_standard_folder, '*'))\n",
    "    predictor_paths = glob.glob(os.path.join(predictor_folder, '*'))\n",
    "    \n",
    "    for g_path in gold_standard_paths:\n",
    "        # load gold standard\n",
    "        print('Loading gold standard from {}'.format(g_path.split('/')[-1]))\n",
    "        gold_standard = csv_to_json(g_path)\n",
    "        print('Done loading gold standard.')\n",
    "        for c_path in predictor_paths:\n",
    "            # load each csv file in sourceForPrediction\n",
    "            print('Loading predictor data sources from {}'.format(c_path.split('/')[-1]))\n",
    "            predictor_data_sources = csv_to_json(c_path)\n",
    "            print('Done loading predictor data sources.')\n",
    "            \n",
    "            # filter gold standard and predictor data sources\n",
    "            gold_standard = filter_data(gold_standard,\n",
    "                                        date_start=date_start,\n",
    "                                        date_end=date_end)\n",
    "            predictor_data_sources = filter_data(predictor_data_sources,\n",
    "                                                 date_start=date_start,\n",
    "                                                 date_end=date_end)\n",
    "            print('Done filtering data.')\n",
    "            \n",
    "            # out-of-sample prediction\n",
    "            prediction = oos_prediction(gold_standard[0],\n",
    "                                        predictor_data_sources,\n",
    "                                        train_period=train_period,\n",
    "                                        test_period=test_period)\n",
    "\n",
    "            # get name of gold standard csv and predictor data csv names.\n",
    "            g_name = g_path.split('/')[-1].split('.')[0]\n",
    "            c_name = c_path.split('/')[-1].split('.')[0]\n",
    "            \n",
    "            # write to csv\n",
    "            if not os.path.exists(save_folder):\n",
    "                os.makedirs(save_folder)\n",
    "\n",
    "            # save oos prediction results\n",
    "            df = pd.DataFrame(columns=['times', 'values'])\n",
    "            df['times'] = prediction['times']\n",
    "            df['values'] = prediction['values']\n",
    "            path = os.path.join(save_folder, '{}_{}_OOSprediction.csv'.format(g_name, c_name))\n",
    "            df.to_csv(path)\n",
    "                                \n",
    "            print('Done out-of-sample prediction using gold standard from {} and predictor data from {}.\\n'.format(g_path.split('/')[-1], c_path.split('/')[-1]))\n",
    "    \n",
    "    print('Done all OOS prediction!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gold standard from GoldStandard\\AMAZONAS-VE.csv\n",
      "Done loading gold standard.\n",
      "Loading predictor data sources from SourcesForPrediction\\Colombia.csv\n",
      "Done loading predictor data sources.\n",
      "Done filtering data.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'optimization.situational_awareness' has no attribute 'pred_OOS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-9141c0493ce8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m      \u001b[0mtrain_period\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'2010-11-22'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2012-09-19'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m      \u001b[0mtest_period\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'2012-09-19'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2013-09-19'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m      save_folder='OOSPredictionResults')\n\u001b[0m",
      "\u001b[1;32m<ipython-input-52-fe73a3bd41e4>\u001b[0m in \u001b[0;36moos_main\u001b[1;34m(gold_standard_folder, predictor_folder, date_start, date_end, train_period, test_period, save_folder)\u001b[0m\n\u001b[0;32m     57\u001b[0m                                         \u001b[0mpredictor_data_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                                         \u001b[0mtrain_period\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_period\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                                         test_period=test_period)\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;31m# get name of gold standard csv and predictor data csv names.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-fe73a3bd41e4>\u001b[0m in \u001b[0;36moos_prediction\u001b[1;34m(gold_standard, predictor_data_sources, train_period, test_period)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \"\"\"\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# predictions based on optimum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     prediction = sa.pred_OOS(gold_standard,\n\u001b[0m\u001b[0;32m     18\u001b[0m                              \u001b[0mpredictor_data_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                              \u001b[0mtrain_period\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'optimization.situational_awareness' has no attribute 'pred_OOS'"
     ]
    }
   ],
   "source": [
    "oos_main(gold_standard_folder='GoldStandard',\n",
    "     predictor_folder='SourcesForPrediction',\n",
    "     date_start='2010-11-22',\n",
    "     date_end='2013-09-19',\n",
    "     train_period=['2010-11-22', '2012-09-19'],\n",
    "     test_period=['2012-09-19', '2013-09-19'],\n",
    "     save_folder='OOSPredictionResults')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots should be generated based on the outputs. Two suggestions here:\n",
    "\n",
    "- a time sereis plot showing gold standard and prediction.\n",
    "\n",
    "- a plot showing how R^2 changes with the size of optimum.\n",
    "\n",
    "You can also generate other plots or tables which you think can make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals of the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two goals:\n",
    "\n",
    "- identify what data sources are good predictors for dengue in Venezuela in national-level.\n",
    "\n",
    "- identify what data sources are good predictors for dengue in Venezuela's states along the border."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
